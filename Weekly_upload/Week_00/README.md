# ðŸ“š Week_00 è®ºæ–‡ç²¾é€‰

## [qwen-vl](qwen-vl.pdf)

**ðŸ“ æ‘˜è¦**ï¼š

> We introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs)
designed to perceive and understand both text and images. Comprising Qwen-VL and
Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image caption-
ing, question answering, visual localization, and flexible interaction. The evaluation covers
a wide range of tasks including zero-shot captioning, visual or document visual question
answering, and grounding. We demonstrate the Qwen-VL outperforms existing LVLMs.
We present their architecture, training, capabilities, and performance, highlighting their
contributions to advancing multimodal artificial intelligence.
Figure 1: Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other
generalist models.
âˆ—Equal contribution, â€ Corresponding author
1
arXiv:2308.12966v2  [cs.CV]  14 Sep 2023
Figure 2: Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple
image inputs, multi-roun

<table><tr>
  <td><img src="./qwen-vl_page1.png" alt="Page image" width="500"/></td>
  <td><img src="./qwen-vl_page2.png" alt="Page image" width="500"/></td>
</tr></table>

