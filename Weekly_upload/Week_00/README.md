# 📚 Week_00 论文精选

## [qwen-vl](qwen-vl.pdf)

**📝 摘要**：

> We introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs)
designed to perceive and understand both text and images. Comprising Qwen-VL and
Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image caption-
ing, question answering, visual localization, and flexible interaction. The evaluation covers
a wide range of tasks including zero-shot captioning, visual or document visual question
answering, and grounding. We demonstrate the Qwen-VL outperforms existing LVLMs.
We present their architecture, training, capabilities, and performance, highlighting their
contributions to advancing multimodal artificial intelligence.
Figure 1: Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other
generalist models.
∗Equal contribution, †Corresponding author
1
arXiv:2308.12966v2  [cs.CV]  14 Sep 2023
Figure 2: Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple
image inputs, multi-roun

<table><tr>
  <td><img src="./qwen-vl_page1.png" alt="Page image" width="500"/></td>
  <td><img src="./qwen-vl_page2.png" alt="Page image" width="500"/></td>
</tr></table>

